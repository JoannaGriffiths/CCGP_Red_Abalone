---
title: "Filtering VCF and SDR Identification and Removal"
author: "Joanna Griffiths"
date: "2024-03-28"
output: 
  html_document:
    toc: true
---

This project starts with a VCF that was generated by CCGP, follwing the SnpArher snakemake pipeline which creates a VCF from raw reads:
https://github.com/cademirch/ccgp_workflow #snpArcher steps broken down
https://github.com/harvardinformatics/snpArcher #snpArcher home

As part of the pipeline, a basic set of filters were already applied to the vcf:
`bcftools_viewCommand=view -S ccgp-workflow-results/73-Haliotis/results/73-Haliotis/GCA_023055435.1/06_QC/73-Haliotis_GCA_023055435.1.samps.txt -t ^mtDNA -v snps -m2 -M2 -f .,PASS -e 'AF==1 | AF==0 | AF<0.01 | ALT="*" | F_MISSING > 0.75 | TYPE~"indel" | ref="N"' -O z -o ccgp-workflow-results/73-Haliotis/results/73-Haliotis/GCA_023055435.1/73-Haliotis_GCA_023055435.1.filtered.vcf.gz ccgp-workflow-results/73-Haliotis/results/73-Haliotis/GCA_023055435.1/73-Haliotis_GCA_023055435.1.final.vcf.gz; `


And here is a basic description of some of these flags from this website:
https://samtools.github.io/bcftools/bcftools.html#view
-m, --min-alleles INT
print sites with at least INT alleles listed in REF and ALT columns

-M, --max-alleles INT
print sites with at most INT alleles listed in REF and ALT columns. Use -m2 -M2 -v snps to only view biallelic SNPs.

-f, --apply-filters LIST
Skip sites where FILTER column does not contain any of the strings listed in LIST. For example, to include only sites which have no filters set, use -f .,PASS.

-e, --exclude EXPRESSION
exclude sites for which EXPRESSION is true. For valid expressions see EXPRESSIONS:
F_MISSING Number:1 Type:Float .. Fraction of missing genotypes
AF Number:A Type:Float .. Allele frequency
MAF Number:A Type:Float .. Minor Allele frequency
TYPE Number:. Type:String .. The record type (REF,SNP,MNP,INDEL,etc)
"*" for any element

-O, --output-type b|u|z|v[0-9]
Output compressed BCF (b), uncompressed BCF (u), compressed VCF (z), uncompressed VCF (v). Use the -Ou option when piping between bcftools subcommands to speed up performance by removing unnecessary compression/decompression and VCF←→BCF conversion.   The compression level of the compressed formats (b and z) can be set by by appending a number between 0-9.

-S, --samples-file [^]FILE
File of sample names to include or exclude if prefixed with "^". One sample per line. See also the note above for the -s, --samples option. The sample order is updated to reflect that given in the input file. The command bcftools call accepts an optional second column indicating ploidy (0, 1 or 2) or sex (as defined by --ploidy, for example "F" or "M"), for example:

-t, --targets [^]chr|chr:pos|chr:from-to|chr:from-[,…​]
Similar as -r, --regions, but the next position is accessed by streaming the whole VCF/BCF rather than using the tbi/csi index. Both -r and -t options can be applied simultaneously: -r uses the index to jump to a region and -t discards positions which are not in the targets. Unlike -r, targets can be prefixed with "^" to request logical complement. For example, "^X,Y,MT" indicates that sequences X, Y and MT should be skipped. Yet another difference between the -t/-T and -r/-R is that -r/-R checks for proper overlaps and considers both POS and the end position of an indel, while -t/-T considers the POS coordinate only (by default; see also --regions-overlap and --targets-overlap). Note that -t cannot be used in combination with -T.

Also note that these individuals are missing from the final vcf file because of lots of missing data (ost likely from low DNA yields:

* AWJG02C12: H8 Monterey (library 1.71ng/uL) lots of missing data so not included in vcf (see QC report)
* AWJG03A07: H12 Monterey (library 0.917ng/uL) lots of missing data so not included in vcf (see QC report)
* AWJG03A12: H13 Monterey (library 2.28ng/uL) lots of missing data so not included in vcf (see QC report)
* AWJG01B07: ENS2 Ensenada (library 1.47ng/uL) lots of missing data so not included in vcf (see QC report)
* AWJG03G10: SJ6 PuntaSanJose (library 1.19ng/uL)

# Filtering

This is a large file that requires a lot of filtering before we can do any analyses.
Many of these filtering steps came from this great workflow: https://www.ddocent.com/filtering/
First, I'm going to remove annotations to make it smaller, using the bash script: `sbatch filter_vcf.sh`


## Remove related individuals
Related individuals were identified using King relatedness matrix in plink, which came from Fig. 7 in the QC report from Erik Enbody.
"Fig. 7: We build a king relatedness matrix using Plink to evaluate if there are closely related samples. Samples with a relatedness of 0.5 indicate they are identical, values close to 0.25 indicate parent-child or sibling relatedness, and second degree relatedness are ~0.125. You may want to remove samples with > 0.354 if you want to remove very closely related sample which can bias population genomic estimates."

* AWJG02B10 (SF14-34-9; MunizRanch) & AWJG02B08 (SF10-15-8; BodegaBay) = 0.39 (on edge, but not really outliers in green on PCA QC report)
* AWJG03D11 (SJ 7; PuntaSanJose) & AWJG02E07 (SJ 8	PuntaSanJose) = 0.39 (also blue outliers in PCA QC report)
* AWJG03H07 (PL7	PointLoma) & AWJG02G11 (PL9	PointLoma) = 0.42 (also blue outliers in PCA QC report)
* AWJG03D12 (SF07-3-12	VanDamme) & AWJG01A02 (SF07-3-10	VanDamme) = 0.35 (also purple outliers in PCA QC report)
* AWJG03F10 (SF98-33-2	Shelter Cover) & AWJG03F09 (SF98-33-1	Shelter Cover) = 0.34 (green outliers in PCA QC report)
* AWJG03G09 (SJ5; PuntaSanJose) & AWJG02E10 (SJ9; PuntaSanJose) = 0.1545
* AWJG03E07 (SF07-7-7; Timber Cove) & AWJG03D09 ( SF11-27-7; Salt Point) = 0.2088
* AWJG03B08 (PL1; Point Loma) & AWJG03G02 (PL12; Point Loma) = 0.216
* AWJG02G06 (PO2133; Port Orford) & AWJG02F12 (PO2134; Port Orford) = 0.029 (coming out as outlier in PCA so removing one)

I decided to remove the following individuals:

* AWJG02B08 because there are fewer samples from Muniz Ranch than Bodega 
* AWJG03D11
* AWJG03H07
* AWJG03D12
* AWJG03F10
* AWJG03G09
* AWJG03E07
* AWJG03B08
* AWJG02G06

adding these to a file called `sibs_to_remove`

Now, let's remove individuals using vcftools: `sbatch filter_vcf3.sh`
```{bash eval=FALSE}
#!/bin/bash

#SBATCH -D /home/jsgriffi
#SBATCH --job-name=filter
#SBATCH --mem=2G
#SBATCH --ntasks=8
#SBATCH -o /group/awhitehegrp/joanna/73-Haliotis/output_files/out-%A.%a_filter.txt
#SBATCH -e /group/awhitehegrp/joanna/73-Haliotis/output_files/error-%A.%a_filter.txt
#SBATCH --time=120:00:00
#SBATCH --mail-user=jsgriffiths@ucdavis.edu
#SBATCH --mail-type=ALL
#SBATCH -p high

cd /group/awhitehegrp/joanna/73-Haliotis

module load deprecated/bio/1.0

vcftools --vcf 73-Haliotis_GCA_023055435.1.filtered.noAnnotations.vcf --remove sibs_to_remove --recode --recode-INFO-all --out NoSibs.filtered.noAnnotations.vcf
```

From output files:
```
After filtering, kept 245 out of 253 Individuals
Outputting VCF file...
After filtering, kept 43541980 out of a possible 43541980 Sites
```

## Remove Low Depth Samples
After running through all the filtering steps and creating a PCA, my Monterey samples keep grouping together, probably because they have low sequencing depth (see figures in powerpoints):
* AWJG01C02 (Pt Reyes SF03-19-2) Mean depth 2.17 and 70% mapping
* AWJG02C08 (H5 Monterey) Mean depth 2.84
* AWJG02B05 (H14 Monterey) Mean depth 2.24
* AWJG02D04 (H6 Monterey) Mean depth 3.67
* AWJG03C04 (H10 Monterey) Mean depth 2.28
* AWJG01D11 (H4 Monterey) Mean depth 2.27
* AWJG03C05 (H11 Monterey) Mean depth 2.27
* AWJG01D06 (H2 Monterey) Mean depth 2.18

These samples came to me as extracted DNA, and I had trouble creating good libraries for them. Even after I filter for minimum depth of 3, they are still grouping together so let's remove all Monterey samples collected in 2011 and the one Pt Reyes sample. I'm keeping the three Monterey samples collected in 1998 for now. This is in a file called `lowDP_samples`:
* AWJG01C02
* AWJG01D06
* AWJG01D11
* AWJG02D04
* AWJG02B05
* AWJG02C08
* AWJG02C12
* AWJG03C04
* AWJG03C05
* AWJG03A07
* AWJG03A12

Run the following commands in: `sbatch filter_vcf3.sh`

```{bash eval=FALSE}

vcftools --vcf NoSibs.filtered.vcf.recode.vcf --remove lowDP_samples --recode --recode-INFO-all --out NoSibs.NoLowDP.filtered

```

From output files:
```
Parameters as interpreted:
        --vcf NoSibs.filtered.vcf.recode.vcf
        --exclude lowDP_samples
        --recode-INFO-all
        --out NoSibs.NoLowDP.filtered
        --recode

Excluding individuals in 'exclude' list
After filtering, kept 237 out of 245 Individuals
Outputting VCF file...
After filtering, kept 43541980 out of a possible 43541980 Sites
```


## Remove missing SNPs

Let's check the quality of the remaining samples in the vcf file by running the following lines of code in the `filter_vcf3.sh` file:
```{bash eval=FALSE}
##check quality of output if desired:
vcftools --vcf NoSibs.filtered.noAnnotations.vcf --missing-indv --out NoSibs.filtered.noAnnotations.vcf
vcftools --vcf NoSibs.filtered.noAnnotations.vcf --FILTER-summary --out NoSibs.filtered.noAnnotations.vcf
vcftools --vcf NoSibs.filtered.noAnnotations.vcf --depth --out NoSibs.filtered.noAnnotations.vcf
```
Note that it's good practice to do this after each filtering step

We are going to only keep variants that have been successfully genotyped in 75% of individuals, min seq quality of 30, min depth of 10, max depth of 40. I played around with a few parameters, but ultimately chose these final ones. Note that the min DP of 10 did not remove any ore SNPs than the minDP of 3. MaxDP of 40 also did not remove any more SNPs by itself.

run `sbatch filter_vcf3.sh`
```{bash eval=FALSE}
module load bio/1.0
vcftools --vcf NoSibs.NoLowDP.filtered.recode.vcf --max-missing 0.5 --minQ 30 --maf 0.05 --minDP 3 --recode --recode-INFO-all --out NoSibs.NoLowDP.filtered.vcf.missing5maf5

vcftools --vcf NoSibs.NoLowDP.filtered.vcf.missing5maf5.recode.vcf --minDP 10 --maxDP 40 --recode --recode-INFO-all --out NoSibs.NoLowDP.filtered.vcf.missing5maf5.min10.maxDP40

vcftools --vcf NoSibs.NoLowDP.filtered.vcf.missing5maf5.min10.maxDP40.recode.vcf --max-missing 0.75 --recode --recode-INFO-all --out NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40
```
In the above code, we are keeping SNPs with a minimum quality score of 30 and a minor allele count of 3.
`--max-missing 0.5` tells it to filter genotypes called below 50% (across all individuals)
`--maf 0.05` flag tells it to filter SNPs that have a minor allele frequency less than 5%.
`--recode` flag tells the program to write a new vcf file with the filters 
`--recode-INFO-all` keeps all the INFO flags from the old vcf file in the new one.

there are so many SNPs (43 million) and we're trying to filter this down to only the real SNPs and a workable file.
From the out file:
```
After filtering, kept 236 out of 236 Individuals
Outputting VCF file...
After filtering, kept 22510269 out of a possible 43541980 Sites
```

Playing around with min and max depth:
```
Parameters as interpreted:
        --vcf NoSibs.NoLowDP.filtered.vcf.missing5maf5.recode.vcf
        --recode-INFO-all
        --maxDP 50
        --minDP 5
        --out NoSibs.NoLowDP.filtered.vcf.missing5maf5.min5.maxDP50
        --recode

After filtering, kept 236 out of 236 Individuals
Outputting VCF file...
After filtering, kept 22510269 out of a possible 22510269 Sites
```

```
Parameters as interpreted:
        --vcf NoSibs.NoLowDP.filtered.vcf.missing5maf5.recode.vcf
        --recode-INFO-all
        --maxDP 40
        --minDP 5
        --out NoSibs.NoLowDP.filtered.vcf.missing5maf5.min5.maxDP40
        --recode

After filtering, kept 236 out of 236 Individuals
Outputting VCF file...
After filtering, kept 22510269 out of a possible 22510269 Sites
```

```
Parameters as interpreted:
        --vcf NoSibs.NoLowDP.filtered.vcf.missing5maf5.recode.vcf
        --recode-INFO-all
        --maxDP 40
        --minDP 10
        --out NoSibs.NoLowDP.filtered.vcf.missing5maf5.min10.maxDP40
        --recode

After filtering, kept 236 out of 236 Individuals
Outputting VCF file...
After filtering, kept 22510269 out of a possible 22510269 Sites
```

```
Parameters as interpreted:
        --vcf NoSibs.NoLowDP.filtered.vcf.missing5maf5.min10.maxDP40.recode.vcf
        --recode-INFO-all
        --max-missing 0.75
        --out NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40
        --recode

After filtering, kept 236 out of 236 Individuals
Outputting VCF file...
After filtering, kept 1099451 out of a possible 22510269 Sites
```


## Remove individuals with lots of missing data

Now type following code line by line on command line to create histogram of missing data per individual
```{bash eval=FALSE}
mawk '!/IN/' out.imiss | cut -f5 > totalmissing
gnuplot << \EOF
set terminal dumb size 120, 30
set autoscale
unset label
set title "Histogram of % missing data per individual"
set ylabel "Number of Occurrences"
set xlabel "% of missing data"
#set yr [0:100000]
binwidth=0.01
bin(x,width)=width*floor(x/width) + binwidth/2.0
plot 'totalmissing' using (bin($1,binwidth)):(1.0) smooth freq with boxes
pause -1
EOF
```
Based on this output, all individuals have less than 20% of SNPs missing so we are keeping all individuals in the dataset (no need to remove anymore)



## Filter by LD
Now let's remove SNPs that are in LD to further pair down the number of SNPs. This is important because SNPs that are physically linked may elevate a signal when really only one of them may be significant.

info about plink and LD: https://zzz.bwh.harvard.edu/plink/ld.shtml, https://www.cog-genomics.org/plink/1.9/ld#indep
`where --r2 is squared correlation based on genotypic allele counts`


estimate LD decay using `plink_LD.sh` script:
```{bash eval=FALSE}
#!/bin/bash

#SBATCH -D /home/jsgriffi
#SBATCH --job-name=plink
#SBATCH --mem=8G
#SBATCH --ntasks=8
#SBATCH -o /group/awhitehegrp/joanna/73-Haliotis/output_files/out-%A.%a_plink_LD.txt
#SBATCH -e /group/awhitehegrp/joanna/73-Haliotis/output_files/error-%A.%a_plink_LD.txt
#SBATCH --time=72:00:00
#SBATCH --mail-user=jsgriffiths@ucdavis.edu
#SBATCH --mail-type=ALL
#SBATCH -p high

cd /group/awhitehegrp/joanna/73-Haliotis

module load deprecated/plink/1.90

#create plink files from vcf file
plink --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.recode.vcf --allow-extra-chr --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink

plink --bfile plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink --allow-extra-chr --recode --tab --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink

#get a file of r2 between all SNPs in dataset
plink --file plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink --allow-extra-chr --r2 --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink_r2 --threads 8

#make a list of SNPs with a r2 less than 0.8
plink --file plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink --set-missing-var-ids @:# --allow-extra-chr --indep-pairwise 100 10 0.8 --r2 --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink_indep_pairwise_100_10_0.8 --threads 8

#Now extract these SNPs from the main vcf file and create a new vcf with only SNPs not in LD (by 0.8)
plink --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.recode.vcf --set-missing-var-ids @:# --recode vcf --allow-extra-chr --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink.LDfiltered_0.8 --extract plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink_indep_pairwise_100_10_0.8.prune.in


```

Output from files:
```
After LD filtering: 891867 variants and 236 people pass filters and QC.
```

# Sex Determing region

We first discovered that individuals might be clustering in the PCA by genetic sex by the formation of two distinct genetic clusters with equal mixing from all geographical locations on PC axis 2. To confirm and validate this finding, we identified the regions of the genome responsible for clustering on PC axis 2.

## Validation

run lines from `plink_pca.sh`
```{bash eval=FALSE}
##Run plink_pca with var-wts modifier to get the SNP weights by each PC axis
plink --file plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink.LDfiltered_0.8 --pca var-wts --allow-extra-chr --out PCA/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink.LDfiltered_0.8.pca_snps
```


Bring output into R and create a manhattan plot of SNPs signifcantly contributing to PC axis 2.
```{r eval=FALSE}
library("tidyr")
library("qqman")

setwd("~/UCDavis/Whitehead_lab/CCGP_Abalone/CCGP_analyses/PCAs")
eigenvec <- read.table("NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink.LDfiltered_0.8.pca_snps.eigenvec.var")


##select only PC axis two since thats the axis that the two clusters are separating on
eigenvec <- eigenvec[,c(1,2,4)]

##calculate z scores
eigenvec$z_scores <- (eigenvec$V4-mean(eigenvec$V4))/sd(eigenvec$V4)

##calculate pvalue associated with zscore
eigenvec$pvalue <- pnorm(eigenvec$z_scores, mean = 0, sd = 1, lower.tail = TRUE)

##checking that the mean is 0 for calculation above
hist(eigenvec$z_scores) #yes!

##Changing names to be pretty
colnames(eigenvec) <- c("CHROM", "CHR_SNP", "Weight", "z_scores", "pvalue")

#Rename to scaffold names
GCA_contigs <- read.delim2("C:/Users/joann/OneDrive/Documents/UCDavis/Whitehead_lab/CCGP_Abalone/CCGP_analyses/GO/GCA_contigs.txt", header=T)
colnames(GCA_contigs) = c("CHROM", "SCAF")

PC2_scaf <- merge(eigenvec, GCA_contigs, by ="CHROM")
PC2_scaf$SCAF <- gsub("SCAF_","",as.character(PC2_scaf$SCAF))


eigenvec2 <- separate(data = PC2_scaf, col = "CHR_SNP", into = c("CHR", "SNP"), sep = "\\:", remove = FALSE)
eigenvec2 <- eigenvec2[,c(4,5,6,7,8)]

#Make manhattan plot
eigenvec2$SCAF<- as.numeric(eigenvec2$SCAF)
eigenvec2$SNP<- as.numeric(eigenvec2$SNP)
eigenvec2$pvalue<- as.numeric(eigenvec2$pvalue)
windows()
manhattan(eigenvec2, chr="SCAF", bp="SNP", snp="SNP", p="pvalue", suggestiveline = -log10(0.05/891867), genomewideline = -log10(0.1/891867), logp = T)

```

Next I calculated the FST between individuals in these two genetic clusters. Fst should be higher than the rest of the genome for the same loci that are significantly contributing to PC axis 2.

run `sbatch PC2_pairwise-fst.sh`
```{bash eval=FALSE}
#!/bin/bash

#SBATCH -D /home/jsgriffi
#SBATCH --job-name=fst
#SBATCH --mem=2G
#SBATCH --ntasks=8
#SBATCH -o /group/awhitehegrp/joanna/73-Haliotis/output_files/out-%A.%a_fst_vcf.txt
#SBATCH -e /group/awhitehegrp/joanna/73-Haliotis/output_files/error-%A.%a_fst_vcf.txt
#SBATCH --time=72:00:00
#SBATCH --mail-user=jsgriffiths@ucdavis.edu
#SBATCH --mail-type=ALL
#SBATCH -p high

cd /group/awhitehegrp/joanna/73-Haliotis

module load deprecated/bio/1.0

vcftools --vcf plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.plink.LDfiltered_0.8.vcf --fst-window-size 10000 --fst-window-step 5000 --weir-fst-pop pop_files/PC2_upper_sample_names.txt --weir-fst-pop pop_files/PC2_lower_sample_names.txt --out popgen/pairwise_fst/PC2
```


Graph the manhattan plot in R:
```{r eval=FALSE}
##########PC2 groupings pairwise-FST

setwd("C:/Users/joann/OneDrive/Documents/UCDavis/Whitehead_lab/CCGP_Abalone/CCGP_analyses/PCAs")
PC2_fst <- read.table("PC2.windowed.weir.fst", header=T)

#change chr names to scaffold names
GCA_contigs <- read.delim2("C:/Users/joann/OneDrive/Documents/UCDavis/Whitehead_lab/CCGP_Abalone/CCGP_analyses/GO/GCA_contigs.txt", header=T)
colnames(GCA_contigs) = c("CHROM", "SCAF")

PC2_fst_scaf <- merge(PC2_fst, GCA_contigs, by ="CHROM")
PC2_fst_scaf$SCAF <- gsub("SCAF_","",as.character(PC2_fst_scaf$SCAF))


PC2_fst_scaf$SNP <- PC2_fst_scaf$BIN_START + 5000
PC2_fst_scaf$SCAF<- as.numeric(PC2_fst_scaf$SCAF)
PC2_fst_scaf$SNP<- as.numeric(PC2_fst_scaf$SNP)
PC2_fst_scaf$WEIGHTED_FST<- as.numeric(PC2_fst_scaf$WEIGHTED_FST)

windows()
manhattan(PC2_fst_scaf, chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F)

#zoom in on single scaffolds where sex-determining region is located
windows()
manhattan(subset(PC2_fst_scaf, SCAF== 14), chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F)

scaf14 <- subset(PC2_fst_scaf, SCAF ==14 & BIN_START > 30000000)
windows()
manhattan(scaf14, chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F, xlim = c(33400000, 33600000))

windows()
manhattan(subset(PC2_fst_scaf, SCAF== 33), chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F)

windows()
manhattan(subset(PC2_fst_scaf, SCAF== 60), chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F)

windows()
big_scaf <- subset(PC2_fst_scaf, SCAF > 99)
manhattan(big_scaf, chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F)

windows()
manhattan(subset(PC2_fst_scaf, SCAF== 120), chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F)

windows()
manhattan(subset(PC2_fst_scaf, SCAF== 154), chr="SCAF", bp="SNP", snp="SNP", p="WEIGHTED_FST", logp = F)

```



## Removal of Sex-determining region

These regions had really high FST.
We need to remove the end of the chromosome (JALGQA010000014.1), and the entire chromosome/contig for: JALGQA010000154.1, JALGQA010000060.1, JALGQA010000120.1, JALGQA010000033.1

run lines on: `filter_vcf3.sh`
```{bash eval=FALSE}

vcftools --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.recode.vcf --recode --recode-INFO-all --not-chr "JALGQA010000154.1" --not-chr "JALGQA010000060.1" --not-chr "JALGQA010000120.1" --not-chr "JALGQA010000033.1" --out NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved

vcftools --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved.recode.vcf --recode --recode-INFO-all --not-chr "JALGQA010000014.1" --out NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved2

vcftools --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved.recode.vcf --recode --recode-INFO-all --chr "JALGQA010000014.1" --to-bp 33450000 --out NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved2.5

module load bcftools/1.14
bcftools concat -f sex_region_merge_files -o NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3


```
Contents of sex_region_merge_files: 
`NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved2.recode.vcf.gz
NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved2.5.recode.vcf.gz`

```
Parameters as interpreted:
        --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.recode.vcf
        --not-chr JALGQA010000033.1
        --not-chr JALGQA010000060.1
        --not-chr JALGQA010000120.1
        --not-chr JALGQA010000154.1
        --recode-INFO-all
        --out NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved
        --recode

After filtering, kept 236 out of 236 Individuals
Outputting VCF file...
After filtering, kept 1093529 out of a possible 1099451 Sites
```

## Re-did LD filtering for vcf without sex-determining region

run lines from: `plink_LD.sh`
```{bash eval=FALSE}
module load deprecated/plink/1.90

#create plink files from vcf file
plink --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.vcf --allow-extra-chr --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink#plink --bfile 

plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink --allow-extra-chr --recode --tab --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink

##get a file of r2 between all SNPs in dataset
plink --file plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink --allow-extra-chr --r2 --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink_r2 --threads 8

##make a list of SNPs with a r2 less than 0.8
plink --file plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink --set-missing-var-ids @:# --allow-extra-chr --indep-pairwise 100 10 0.8 --r2 --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink_indep_pairwise_100_10_0.8 --threads 8

##Now extract these SNPs from the main vcf file and create a new vcf with only SNPs not in LD (by 0.8)
plink --vcf NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.vcf --set-missing-var-ids @:# --recode vcf --allow-extra-chr --out plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink.LDfiltered_0.8 --extract plink_LD/NoSibs.NoLowDP.filtered.vcf.missing75maf5.min10.maxDP40.contigremoved3.plink_indep_pairwise_100_10_0.8.prune.in
```

Output files:
after LD filtering for contigremoved2: 853942 variants remaining

after LD filtering for contigremoved3: 881814 variants remaining.

I also did this for contigremoved2 file and found no difference in the PCA with contigremoved3, so contigremoved3 was the final file used.
